[mlp]
layers = [10, 256, 256, 3]
activation = relu
batch_size = 64
k_fold = 2
na_handling_method = average
bacth_size = 16
num_epochs = 1