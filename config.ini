[mlp]
layers = [10, 256, 3]
activation = relu
batch_size = 64
k_fold = 5
na_handling_method = average
bacth_size = 16
num_epochs = 20
batch_norm = True
data_normalization = True