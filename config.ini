[mlp]
layers = [8, 256, 256, 3]
activation = 'relu'
